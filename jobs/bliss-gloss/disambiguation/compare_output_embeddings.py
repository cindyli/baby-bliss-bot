# Usage: python compare_output_embeddings.py <epochs> <learning_rate>

"""
This script calculates the output embedding of the new token meaning the dog bark
using two methods:

1. Optimization based on positive and negative context sentences.
2. Math inversoin of the matrix multiplication of the target logits and the hidden states.

It then compares the output embeddings generated by these two methods to verify their
similarity.
"""

import sys
import os
import time
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch.nn.functional as F
from data import dataset_animal_bark as dataset
from utils import create_training_data, calc_embeddings, optimize_embeddings

epochs = int(sys.argv[1])
learning_rate = float(sys.argv[2])

training_positive_context_sentences = dataset.training_positive_context_sentences
training_negative_context_sentences = dataset.training_negative_context_sentences

# Track the total running time of this script
start_time = time.time()

# Load model and tokenizer
# model_dir = os.path.expanduser("~") + "/Development/LLMs/Llama-3.1-8B-Instruct"
model_dir = os.path.expanduser("~") + "/projects/ctb-whkchun/s2_bliss_LLMs/Llama-3.1-8B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_dir)
model = AutoModelForCausalLM.from_pretrained(model_dir)

# Set the model to evaluation mode
model.eval()

# Get original "bark" token id for training data
target_token = " bark"
tokens = tokenizer.tokenize(target_token)
bark_token_id = tokenizer.convert_tokens_to_ids(tokens)[0]
print("bark_token_id:", bark_token_id)

# Get the input embedding of the target token
target_token_input_embedding = model.get_input_embeddings().weight[bark_token_id]

# Prepare training data
hidden_states, target_logits = create_training_data(
    model, tokenizer, training_positive_context_sentences, training_negative_context_sentences, bark_token_id
)

# Optimize embeddings
embed_dim = model.config.hidden_size
output_emb_from_optimization = optimize_embeddings(
    model, hidden_states, target_logits, embed_dim, epochs, learning_rate
)

end_time_training = time.time()
elapsed_time = end_time_training - start_time
print(f"Execution time for training: {int(elapsed_time // 60)} minutes and {elapsed_time % 60:.2f} seconds")

output_emb_from_calculation = calc_embeddings(hidden_states, target_logits)

# Calculate elapsed time
end_time_calc = time.time()
elapsed_time = end_time_calc - end_time_training
print(f"Execution time for calculation: {int(elapsed_time // 60)} minutes and {elapsed_time % 60:.2f} seconds")

# Calculate cosine similarity between the two output embeddings
cosine_similarity = F.cosine_similarity(output_emb_from_optimization, output_emb_from_calculation, dim=0)
print(f"Cosine similarity between the two output embeddings: {cosine_similarity.item():.4f}")
